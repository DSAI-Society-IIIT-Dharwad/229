{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc08a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running on device: cuda (cuda) ---\n",
      "--- 1. Loading Model and Tokenizer (Standard Load) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/electra-small-discriminator successfully on cuda.\n",
      "\n",
      "--- 2. Loading and Preprocessing Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 14185.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits loaded. Train size (subset): 5000, Test size (subset): 500\n",
      "\n",
      "--- 3. Starting PyTorch Fine-Tuning for 5 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:10<00:00, 29.01it/s, loss=0.815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|██████▉   | 219/313 [00:07<00:03, 30.05it/s, loss=0.731]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    117\u001b[39m loss = outputs.loss\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[32m    123\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shreyas\\anaconda3\\envs\\torch_nightly\\Lib\\site-packages\\torch\\_tensor.py:628\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    620\u001b[39m         Tensor.backward,\n\u001b[32m    621\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    626\u001b[39m         inputs=inputs,\n\u001b[32m    627\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shreyas\\anaconda3\\envs\\torch_nightly\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shreyas\\anaconda3\\envs\\torch_nightly\\Lib\\site-packages\\torch\\autograd\\graph.py:857\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    855\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This script outlines the emergency process for fast classification fine-tuning.\n",
    "# It uses a small non-quantized model and a pure PyTorch loop to avoid the\n",
    "# 'accelerate' dependency and the resulting ImportError.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm \n",
    "import os \n",
    "\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "MODEL_NAME = \"google/electra-small-discriminator\" # FAST, SMALL model\n",
    "DATASET_NAME = \"tweet_eval\" \n",
    "TASK_NAME = \"sentiment\"    \n",
    "OUTPUT_DIR = \"./results_sentiment_hackathon_emergency\" \n",
    "LABELS = ['negative', 'neutral', 'positive'] \n",
    "NUM_EPOCHS = 5 \n",
    "TRAIN_BATCH_SIZE = 16 \n",
    "LEARNING_RATE = 2e-5 \n",
    "\n",
    "# --- GPU/Device Configuration (Crucial for Local GPU Use) ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE_STR = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(f\"--- Running on device: {DEVICE} ({DEVICE_STR}) ---\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. Load Model and Tokenizer (Standard Load) ---\n",
    "print(\"--- 1. Loading Model and Tokenizer (Standard Load) ---\")\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(LABELS),\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(DEVICE)\n",
    "    print(f\"Loaded {MODEL_NAME} successfully on {DEVICE_STR}.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR LOADING MODEL: {e}\")\n",
    "    print(\"Could not load base model. Check internet or model name.\")\n",
    "\n",
    "\n",
    "# --- 2. Load and Prepare Dataset ---\n",
    "print(\"\\n--- 2. Loading and Preprocessing Dataset ---\")\n",
    "def load_and_prepare_dataset(tokenizer):\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME, TASK_NAME)\n",
    "        # FIX: Ensure no invisible characters here\n",
    "        label_mapping = {0: 0, 1: 1, 2: 2} \n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            tokenized_examples = tokenizer(\n",
    "                examples['text'], \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                max_length=128\n",
    "            )\n",
    "            tokenized_examples[\"labels\"] = [label_mapping[label] for label in examples[\"label\"]]\n",
    "            return tokenized_examples\n",
    "\n",
    "        tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Use a small subset for a fast hackathon run\n",
    "        train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000)).remove_columns(['text', 'label'])\n",
    "        eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)).remove_columns(['text', 'label'])\n",
    "        \n",
    "        # Set format for PyTorch DataLoader\n",
    "        train_dataset.set_format(\"torch\")\n",
    "        eval_dataset.set_format(\"torch\")\n",
    "\n",
    "        print(f\"Dataset splits loaded. Train size (subset): {len(train_dataset)}, Test size (subset): {len(eval_dataset)}\")\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preprocessing dataset: {e}\")\n",
    "        return None, None\n",
    "\n",
    "train_data, eval_data = load_and_prepare_dataset(tokenizer)\n",
    "\n",
    "# --- 3. PyTorch Training Loop (REPLACES Trainer) ---\n",
    "if train_data and eval_data:\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"\\n--- 3. Starting PyTorch Fine-Tuning for {NUM_EPOCHS} epochs ---\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move tensors to the device\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    # --- 4. Save Fine-Tuned Model ---\n",
    "    print(\"\\n--- 4. Saving Model ---\")\n",
    "    model.save_pretrained(f\"{OUTPUT_DIR}/final_fine_tuned_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_fine_tuned_model\")\n",
    "    print(\"\\nFine-Tuning complete and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e21599c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running inference on device: cuda ---\n",
      "Inference model loaded successfully from ./results_sentiment_hackathon_emergency/final_fine_tuned_model.\n",
      "\n",
      "--- Inference Results ---\n",
      "--------------------------------------------------\n",
      "Text: \"This hackathon is the absolute best; I've learned so much!\"\n",
      "CLASSIFICATION: POSITIVE (Confidence: 0.9732)\n",
      "Detail: {'negative': '0.0074', 'neutral': '0.0194', 'positive': '0.9732'}\n",
      "--------------------------------------------------\n",
      "Text: \"The server crashed right before the deadline. So frustrating.\"\n",
      "CLASSIFICATION: NEGATIVE (Confidence: 0.8886)\n",
      "Detail: {'negative': '0.8886', 'neutral': '0.0934', 'positive': '0.0180'}\n",
      "--------------------------------------------------\n",
      "Text: \"I guess the results are fine, nothing spectacular, nothing bad.\"\n",
      "CLASSIFICATION: POSITIVE (Confidence: 0.9246)\n",
      "Detail: {'negative': '0.0191', 'neutral': '0.0562', 'positive': '0.9246'}\n",
      "--------------------------------------------------\n",
      "Text: \"That's the worst error I've ever seen.\"\n",
      "CLASSIFICATION: NEGATIVE (Confidence: 0.8757)\n",
      "Detail: {'negative': '0.8757', 'neutral': '0.1068', 'positive': '0.0175'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "OUTPUT_DIR = \"./results_sentiment_hackathon_emergency/final_fine_tuned_model\"\n",
    "LABELS = ['negative', 'neutral', 'positive'] \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Running inference on device: {DEVICE} ---\")\n",
    "\n",
    "# --- 1. Load Model and Tokenizer ---\n",
    "def load_inference_model(output_path):\n",
    "    \"\"\"Loads the fine-tuned model and tokenizer from the saved path.\"\"\"\n",
    "    try:\n",
    "        # Load the tokenizer and model from the saved directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(output_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(output_path)\n",
    "        model.to(DEVICE)\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"Inference model loaded successfully from {output_path}.\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference model: {e}\")\n",
    "        print(\"Ensure the training script finished successfully and the directory path is correct.\")\n",
    "        return None, None\n",
    "\n",
    "model, tokenizer = load_inference_model(OUTPUT_DIR)\n",
    "\n",
    "# --- 2. Inference Function ---\n",
    "def classify_text(texts, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"Tokenizes a list of texts and returns the predicted label and score.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return [\"Error: Model not loaded\"] * len(texts)\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    encoding = tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move tensors to the appropriate device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get probabilities (softmax) and predicted classes\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    predictions = []\n",
    "    for probs in probabilities:\n",
    "        # Get the index of the highest probability\n",
    "        pred_index = np.argmax(probs)\n",
    "        # Get the label and confidence score\n",
    "        label = LABELS[pred_index]\n",
    "        score = probs[pred_index]\n",
    "        \n",
    "        predictions.append({\n",
    "            \"text\": texts[probabilities.tolist().index(probs.tolist())], # Retrieve the original text\n",
    "            \"prediction\": label,\n",
    "            \"confidence\": f\"{score:.4f}\",\n",
    "            \"probabilities\": {l: f\"{p:.4f}\" for l, p in zip(LABELS, probs)}\n",
    "        })\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# --- 3. Example Execution ---\n",
    "if model and tokenizer:\n",
    "    example_texts = [\n",
    "        \"This hackathon is the absolute best; I've learned so much!\",\n",
    "        \"The server crashed right before the deadline. So frustrating.\",\n",
    "        \"I guess the results are fine, nothing spectacular, nothing bad.\",\n",
    "        \"That's the worst error I've ever seen.\",\n",
    "    ]\n",
    "\n",
    "    results = classify_text(example_texts, model, tokenizer, DEVICE)\n",
    "\n",
    "    print(\"\\n--- Inference Results ---\")\n",
    "    for result in results:\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Text: \\\"{result['text']}\\\"\")\n",
    "        print(f\"CLASSIFICATION: {result['prediction'].upper()} (Confidence: {result['confidence']})\")\n",
    "        print(f\"Detail: {result['probabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f06de1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running on device: cuda ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:11<00:00, 4491.21 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(OUTPUT_DIR):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         trained_model, trained_tokenizer = \u001b[43mrun_lora_fine_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel already fine-tuned, loading from disk...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_lora_fine_tuning\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m metric.compute(predictions=preds, references=eval_pred.label_ids, average=\u001b[33m\"\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# --- 5. Training setup ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# disables W&B or TensorBoard\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# --- 6. Trainer ---\u001b[39;00m\n\u001b[32m     82\u001b[39m     trainer = Trainer(\n\u001b[32m     83\u001b[39m         model=model,\n\u001b[32m     84\u001b[39m         args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m         compute_metrics=compute_metrics,\n\u001b[32m     88\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"./lora_finetuned_model\"\n",
    "NUM_LABELS = 2\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "\n",
    "def run_lora_fine_tuning():\n",
    "    print(\"--- Running on device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\", \"---\")\n",
    "\n",
    "    # --- 1. Load model and tokenizer ---\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # --- 2. Apply LoRA ---\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=[\"q_lin\", \"v_lin\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # --- 3. Load dataset (example: IMDb) ---\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    tokenized = dataset.map(tokenize, batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(\"torch\")\n",
    "\n",
    "    train_data = tokenized[\"train\"]\n",
    "    eval_data = tokenized[\"test\"]\n",
    "\n",
    "    # --- 4. Metric ---\n",
    "    metric = evaluate.load(\"f1\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return metric.compute(predictions=preds, references=eval_pred.label_ids, average=\"weighted\")\n",
    "\n",
    "    # --- 5. Training setup ---\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[],  # disables W&B or TensorBoard\n",
    ")\n",
    "\n",
    "\n",
    "    # --- 6. Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "    print(\"✅ Fine-tuning complete!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        trained_model, trained_tokenizer = run_lora_fine_tuning()\n",
    "    else:\n",
    "        print(\"Model already fine-tuned, loading from disk...\")\n",
    "        trained_model = AutoModelForSequenceClassification.from_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "        trained_tokenizer = AutoTokenizer.from_pretrained(f\"{OUTPUT_DIR}/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e198a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
